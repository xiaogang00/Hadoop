**数据流** 

MapReduce作业是客户端需要执行的一个工作单元，它包括输入数据，MapReduce程序和配置信息。Hadoop将作业分成若干个小任务。其中包括两类任务：map任务，reduce任务。

有两类节点控制着作业的执行过程，一个是jobtracker以及一系列的tasktracker。jobtracker通过调度tasktracker在运行的任务来协调所有运行在系统上的作业。

Hadoop将MapReduce的输入数据划分成等长的数据块。这些数据块称为是输入分片或者是分片。Hadoop为每个分片构建一个map任务。

对于一个合理的系统来说，分片的大小应该趋向于HDFS的一个块的大小。