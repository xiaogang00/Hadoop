**数据流** 

MapReduce作业是客户端需要执行的一个工作单元，它包括输入数据，MapReduce程序和配置信息。Hadoop将作业分成若干个小任务。其中包括两类任务：map任务，reduce任务。

有两类节点控制着作业的执行过程，一个是jobtracker以及一系列的tasktracker。jobtracker通过调度tasktracker在运行的任务来协调所有运行在系统上的作业。

Hadoop将MapReduce的输入数据划分成等长的数据块。这些数据块称为是输入分片或者是分片。Hadoop为每个分片构建一个map任务。

对于一个合理的系统来说，分片的大小应该趋向于HDFS的一个块的大小。



HDFS的设计：

* 超大文件
* 流式数据访问，HDFS的构建思路：一次写入，多次读取是最高效的访问模式。数据集通常是由数据源生成或者从数据源复制而来。长时间在此数据集上进行各种分析。每次分析都将设计该数据的大部分数据。
* 商用硬件
* 低时间延迟的数据访问
* 大量的小文件
* 多用户写入，任意修改文件