**数据流** 

MapReduce作业是客户端需要执行的一个工作单元，它包括输入数据，MapReduce程序和配置信息。Hadoop将作业分成若干个小任务。其中包括两类任务：map任务，reduce任务。

有两类节点控制着作业的执行过程，一个是jobtracker以及一系列的tasktracker。jobtracker通过调度tasktracker在运行的任务来协调所有运行在系统上的作业。

Hadoop将MapReduce的输入数据划分成等长的数据块。这些数据块称为是输入分片或者是分片。Hadoop为每个分片构建一个map任务。

对于一个合理的系统来说，分片的大小应该趋向于HDFS的一个块的大小。



HDFS的设计：

* 超大文件
* 流式数据访问，HDFS的构建思路：一次写入，多次读取是最高效的访问模式。数据集通常是由数据源生成或者从数据源复制而来。长时间在此数据集上进行各种分析。每次分析都将设计该数据的大部分数据。
* 商用硬件
* 低时间延迟的数据访问
* 大量的小文件
* 多用户写入，任意修改文件

HDFS中同样也有块(block)的概念，上面的文件被划分为块大小的多个分块(chunk)。但是与其他系统不同的是，HDFS中小于一个块大小的文件不会占据整个块的空间。

HDFS中的fsck命令可以显示块信息：

%hadoop fsck / -files  -blocks



HDFS上有两类节点的管理者以-工作者模式运行

管理者：namenode，管理文件系统的命名空间，维护整个文件系统树以及整棵树内所有文件和目录。这些信息以命名空间镜像文件和编辑日志文件的形式储存在磁盘上

工作者：datanode

